import torch.nn as nn
import torch
import random

class TextCNN(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.kernels = config.kernels
        self.num_labels = config.num_labels
        self.hidden_size = config.hidden_size

        self.CNNModel = CNNModel(config)
        self.drop = nn.Dropout(config.dropout)
        self.activation = nn.ReLU()
        self.fc = nn.Linear(len(config.kernels)*config.num_features, config.num_labels)

    def forward(self, input_ids):
        features = self.CNNModel(input_ids)
        features = self.activation(features)
        features = self.drop(features)
        logits = self.fc(features)

        return logits

class CNNModel(nn.Module):
    def __init__(self, config):
        super(CNNModel, self).__init__()
        self.kernels = config.kernels

        if config.embedding_pretrained:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_path, freeze=False)
        else:
            self.embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.vocab_size - 1)

        self.CNNLayers = nn.ModuleList(
            [CNN(kernel, config.hidden_size, config.max_len, config.num_features) for kernel in config.kernels])

    def forward(self, input_ids):
        embedding = self.embedding(input_ids)
        features = torch.cat([self.CNNLayers[i](embedding) for i in range(len(self.kernels))], 1)

        return features
"""
acquire the character value (as singular) generated by specified kernel size
"""
class CNN(nn.Module):
    def __init__(self, kernel, hidden_size, max_len, num_kernel):
        super(CNN, self).__init__()
        self.kernel = kernel
        self.hidden_size = hidden_size
        self.conv = nn.Conv2d(1, num_kernel, kernel_size=[kernel, hidden_size])
        self.pool = nn.MaxPool1d(kernel_size=max_len-kernel+1)

    def forward(self, embedding):
        embedding = embedding.unsqueeze(1)
        features = self.conv(embedding)
        features = torch.squeeze(features, -1)
        pooled = self.pool(features)
        feature = torch.squeeze(pooled, -1)

        return feature



class GANTextCNN(nn.Module):
    def __init__(self, CFG):
        super(GANTextCNN, self).__init__()
        self.config = CFG
        self.num_fakelabels = len(CFG.fake_labels)
        self.hidden_dim = CFG.num_features
        self.num_labels = CFG.num_labels + 1
        self.fixGenerator = True

        self.textCNN = CNNModel(CFG)
        self.generator = Generator(CFG)
        self.discriminator = Discrimitor(CFG)

    def forward(self, input_ids=None, labels=None):
        if self.training:
            fakeSample, fakeLabels = self.generator()

            if self.fixGenerator:
                fakeLabels = (self.num_labels-1)*torch.ones(self.num_fakelabels, dtype=torch.int).to(self.config.device)
                realSample = self.textCNN(input_ids)

                samples = torch.cat((fakeSample, realSample), dim=0)
                labels = torch.cat((fakeLabels, labels), dim=0)

                logits = self.discriminator(samples)

                return logits, labels
            else:
                logits = self.discriminator(fakeSample)
                return logits, fakeLabels
        else:
            realSample = self.textCNN(input_ids)
            logits = self.discriminator(realSample)
            return logits


class Generator(nn.Module):
    def __init__(self, CFG):
        super(Generator, self).__init__()
        self.config = CFG
        self.labels = CFG.fake_labels
        self.input_dim = CFG.max_len
        self.hidden_dim = CFG.num_features*len(CFG.kernels)

        self.fc = nn.Linear(self.input_dim, self.hidden_dim)

    def generateFakeInputs(self):
        """
        generate the random inputs of Generator
        :return: size of [num_labels, input_dim]
        """

        noise = torch.normal(mean=0, size=(len(self.labels), self.input_dim), std=0.5)
        inputs = torch.ones(size=(len(self.labels), self.input_dim), dtype=torch.float32)
        inputs = torch.tensor(self.labels).reshape((len(self.labels),-1))*inputs

        inputs += noise
        inputs = inputs.cuda(self.config.device)
        return inputs

    def forward(self):

        random.shuffle(self.labels)
        inputs = self.generateFakeInputs()

        embeddings = self.fc(inputs)

        labels = torch.tensor(self.labels).cuda(self.config.device)
        return embeddings, labels


class Discrimitor(nn.Module):
    def __init__(self, CFG):
        super(Discrimitor, self).__init__()
        self.config = CFG
        self.hidden_dim = CFG.num_features*len(CFG.kernels)
        self.num_labels = CFG.num_labels+1

        self.drop = nn.Dropout(CFG.dropout)
        self.activation = nn.ReLU()
        self.fc = nn.Linear(self.hidden_dim, self.num_labels)

    def forward(self, batch):
        output = self.activation(batch)
        output = self.drop(output)
        output = self.fc(output)
        return output